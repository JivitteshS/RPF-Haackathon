{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fae69a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from typing import TypedDict, List, Annotated\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import operator\n",
    "from langgraph.graph import END\n",
    "import os\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a3f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://expazure-openai.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"0f5cb3c2ad3d40d9a76142c3f49cad9f\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt35\",\n",
    "    api_version=\"2024-02-01\",\n",
    "    temperature=0,\n",
    "    max_tokens=2000,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10973d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-02-01\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://expazure-openai.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"0f5cb3c2ad3d40d9a76142c3f49cad9f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a2ead13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"embeddingada\",\n",
    "    openai_api_version=\"2024-02-01\",\n",
    ")\n",
    "\n",
    "vs_2=FAISS.load_local(f\"Vector_data\\Install,Upgrade and hotflix StorageGRID\", embeddings,allow_dangerous_deserialization=True)\n",
    "retriever_2 = vs_2.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e8b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_7=FAISS.load_local(f\"Vector_data\\Maintain a Storage Grid\",embeddings,allow_dangerous_deserialization=True)\n",
    "retriever_7 = vs_7.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017cc307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from typing import Literal\n",
    "retriever_tool_2 = create_retriever_tool(\n",
    " \n",
    "retriever = retriever_2,\n",
    "name = \"Install Upgrade and hotflix StorageGRID\",\n",
    "description =\"\"\"\n",
    "    The document provides detailed instructions on installing, upgrading, and applying hotfixes to \n",
    "    StorageGRID, focusing on Red Hat Enterprise Linux. It covers preparation steps, deployment, and \n",
    "    configuration of grid nodes, both software-based and appliance nodes. Automation options using orchestration\n",
    "    frameworks and Python scripts are discussed. It also outlines required information, materials, and hardware\n",
    "    specifications, emphasizing performance, storage, and network requirements. Additionally, \n",
    "    it includes guidelines for downloading and extracting necessary \n",
    "    files and managing StorageGRID nodes and resources effectively.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "retriever_tool_7=create_retriever_tool(\n",
    " \n",
    "retriever = retriever_7,\n",
    "name = \"Maintatin a StorageGRID\",\n",
    "description =\"\"\"\n",
    "    The document details how to maintain a StorageGRID system, including tasks such as decommissioning nodes\n",
    "    or sites, renaming elements, and performing host and middleware procedures. It outlines prerequisites for \n",
    "    maintenance, including understanding the system topology and following instructions precisely. \n",
    "    Specific procedures for downloading recovery packages, decommissioning nodes, and considerations for\n",
    "    each node type are provided, emphasizing the importance of careful planning to ensure data integrity\n",
    "    and system performance during maintenance activities.\n",
    "    \"\"\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e36d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Does the solution have automatic failure notification via email, text, and other methods?\"\n",
    "\n",
    "topic='Maintain a StorageGRID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b64b5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool_2,retriever_tool_7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41cb349c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'itemgetter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 38\u001b[0m\n\u001b[0;32m     32\u001b[0m topic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaintain a StorageGRID\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     35\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(PROMPT)\n\u001b[0;32m     37\u001b[0m chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 38\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: itemgetter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m: itemgetter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;241m|\u001b[39m route_prompt\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;241m|\u001b[39m llm_with_tools \n\u001b[0;32m     41\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     45\u001b[0m response_output \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke([{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m:topic}])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'itemgetter' is not defined"
     ]
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "azure_deployment=\"gpt35\",\n",
    "api_version=\"2024-02-01\",\n",
    "temperature=0,\n",
    "max_tokens=2000,\n",
    "timeout=None,\n",
    "max_retries=2,\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "llm_with_tools=llm.bind_tools(tools)\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a helpful assistant in routing the query to the required vertor database for answering the user question\" ),\n",
    "            (\"human\", f\"This is the question: {question}, this is the topic: {topic}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "PROMPT=\"\"\"This is a fake prompt...\n",
    "\n",
    "question: {question}\n",
    "\n",
    "topic: {topic}\n",
    "\n",
    "Result:\"\"\"\n",
    "\n",
    "question=\"Does the solution have automatic failure notification via email, text, and other methods?\"\n",
    "\n",
    "topic='Maintain a StorageGRID'\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(PROMPT)\n",
    "\n",
    "chain = (\n",
    "    {\"question\": itemgetter(\"question\"), \"topic\": itemgetter(\"topic\")}\n",
    "    | route_prompt\n",
    "    | llm_with_tools \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "response_output = chain.invoke([{\"question\": question,\"topic\":topic}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ffb9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    model = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt35\",\n",
    "    api_version=\"2024-02-01\",\n",
    "    temperature=0,\n",
    "    max_tokens=2000,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    )\n",
    "    \n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84070f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9adf3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        vectordb: Name of the vectordb\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    vectordb: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dcfb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a \n",
    "    user question to the required vectorstore  Use the vectorstore for questions on the required topic. You have to be \n",
    "    stringent with the keywords in the question related to these topics. \n",
    "    no premable or explaination. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0ac30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "    source = question_router.invoke({\"question\": question})  \n",
    "    print(source)\n",
    "    print(source['datasource'])\n",
    "    if source['datasource'] == 'web_search':\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source['datasource'] == 'vectorstore':\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "446abe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def RouteQuery(State):\n",
    "    \n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "    \n",
    "    question=State[\"question\"]\n",
    "    topic=State[\"vectordb\"]\n",
    "\n",
    "    datasource: Literal[\"Install Upgrade and hotflix StorageGRID\", \"Maintatin a StorageGRID\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question and topic choose to route it to the required vectorstore to answer the queries based on the topic.\",\n",
    "    )\n",
    "\n",
    "    structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "    \n",
    "    llm_with_tools=structured_llm_router.bind_tools(tools)\n",
    "    \n",
    "    route_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\"human\", f\"This is the question: {question}, this is the topic: {topic}\"),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    \n",
    "\n",
    "    response=structured_llm_router | route_prompt | StrOutputParser()\n",
    "    \n",
    "    response_output = question_rewriter.invoke({\"question\": question,\"topic\":topic})\n",
    "    \n",
    "    return {\"generation\": response_output}\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df26d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow=StateGraph(GraphState)\n",
    "workflow.add_node(\"Route Query\",RouteQuery) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530f13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"Route Query\")\n",
    "workflow.add_edge(\"Analyze Topics\",END)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
